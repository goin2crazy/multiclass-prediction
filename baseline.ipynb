{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60893,"databundleVersionId":7000181,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import modules\nimport pandas as pd \nimport numpy as np \n\nfrom sklearn.preprocessing import (PowerTransformer, \n                                   LabelEncoder)\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-30T11:58:09.099467Z","iopub.execute_input":"2023-12-30T11:58:09.099909Z","iopub.status.idle":"2023-12-30T11:58:09.106490Z","shell.execute_reply.started":"2023-12-30T11:58:09.099873Z","shell.execute_reply":"2023-12-30T11:58:09.105116Z"},"trusted":true},"execution_count":221,"outputs":[]},{"cell_type":"code","source":"# init global variables\ntrain_path = '/kaggle/input/playground-series-s3e26/train.csv'\ntest_path = '/kaggle/input/playground-series-s3e26/test.csv'","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:58:09.108791Z","iopub.execute_input":"2023-12-30T11:58:09.109296Z","iopub.status.idle":"2023-12-30T11:58:09.122890Z","shell.execute_reply.started":"2023-12-30T11:58:09.109260Z","shell.execute_reply":"2023-12-30T11:58:09.122014Z"},"trusted":true},"execution_count":222,"outputs":[]},{"cell_type":"code","source":"# read datasets\ntrain_df = pd.read_csv(train_path)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:58:09.570837Z","iopub.execute_input":"2023-12-30T11:58:09.571986Z","iopub.status.idle":"2023-12-30T11:58:09.622076Z","shell.execute_reply.started":"2023-12-30T11:58:09.571938Z","shell.execute_reply":"2023-12-30T11:58:09.620910Z"},"trusted":true},"execution_count":223,"outputs":[{"execution_count":223,"output_type":"execute_result","data":{"text/plain":"   id  N_Days             Drug    Age Sex Ascites Hepatomegaly Spiders Edema  \\\n0   0     999  D-penicillamine  21532   M       N            N       N     N   \n1   1    2574          Placebo  19237   F       N            N       N     N   \n2   2    3428          Placebo  13727   F       N            Y       Y     Y   \n3   3    2576          Placebo  18460   F       N            N       N     N   \n4   4     788          Placebo  16658   F       N            Y       N     N   \n\n   Bilirubin  Cholesterol  Albumin  Copper  Alk_Phos    SGOT  Tryglicerides  \\\n0        2.3        316.0     3.35   172.0    1601.0  179.80           63.0   \n1        0.9        364.0     3.54    63.0    1440.0  134.85           88.0   \n2        3.3        299.0     3.55   131.0    1029.0  119.35           50.0   \n3        0.6        256.0     3.50    58.0    1653.0   71.30           96.0   \n4        1.1        346.0     3.65    63.0    1181.0  125.55           96.0   \n\n   Platelets  Prothrombin  Stage Status  \n0      394.0          9.7    3.0      D  \n1      361.0         11.0    3.0      C  \n2      199.0         11.7    4.0      D  \n3      269.0         10.7    3.0      C  \n4      298.0         10.6    4.0      C  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>N_Days</th>\n      <th>Drug</th>\n      <th>Age</th>\n      <th>Sex</th>\n      <th>Ascites</th>\n      <th>Hepatomegaly</th>\n      <th>Spiders</th>\n      <th>Edema</th>\n      <th>Bilirubin</th>\n      <th>Cholesterol</th>\n      <th>Albumin</th>\n      <th>Copper</th>\n      <th>Alk_Phos</th>\n      <th>SGOT</th>\n      <th>Tryglicerides</th>\n      <th>Platelets</th>\n      <th>Prothrombin</th>\n      <th>Stage</th>\n      <th>Status</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>999</td>\n      <td>D-penicillamine</td>\n      <td>21532</td>\n      <td>M</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>2.3</td>\n      <td>316.0</td>\n      <td>3.35</td>\n      <td>172.0</td>\n      <td>1601.0</td>\n      <td>179.80</td>\n      <td>63.0</td>\n      <td>394.0</td>\n      <td>9.7</td>\n      <td>3.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2574</td>\n      <td>Placebo</td>\n      <td>19237</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0.9</td>\n      <td>364.0</td>\n      <td>3.54</td>\n      <td>63.0</td>\n      <td>1440.0</td>\n      <td>134.85</td>\n      <td>88.0</td>\n      <td>361.0</td>\n      <td>11.0</td>\n      <td>3.0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>3428</td>\n      <td>Placebo</td>\n      <td>13727</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>Y</td>\n      <td>3.3</td>\n      <td>299.0</td>\n      <td>3.55</td>\n      <td>131.0</td>\n      <td>1029.0</td>\n      <td>119.35</td>\n      <td>50.0</td>\n      <td>199.0</td>\n      <td>11.7</td>\n      <td>4.0</td>\n      <td>D</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2576</td>\n      <td>Placebo</td>\n      <td>18460</td>\n      <td>F</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>N</td>\n      <td>0.6</td>\n      <td>256.0</td>\n      <td>3.50</td>\n      <td>58.0</td>\n      <td>1653.0</td>\n      <td>71.30</td>\n      <td>96.0</td>\n      <td>269.0</td>\n      <td>10.7</td>\n      <td>3.0</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>788</td>\n      <td>Placebo</td>\n      <td>16658</td>\n      <td>F</td>\n      <td>N</td>\n      <td>Y</td>\n      <td>N</td>\n      <td>N</td>\n      <td>1.1</td>\n      <td>346.0</td>\n      <td>3.65</td>\n      <td>63.0</td>\n      <td>1181.0</td>\n      <td>125.55</td>\n      <td>96.0</td>\n      <td>298.0</td>\n      <td>10.6</td>\n      <td>4.0</td>\n      <td>C</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class GetDummies:\n    def __init__(self, name='category', fit=None):\n        self.columns = None\n        self.fillna_value = None\n        self.name = name\n        \n        if fit is not None: \n            self.fit(fit)\n            self.first_fit = True\n        else: \n            self.first_fit = False\n            \n    def fit(self, series):\n        self.columns = series.unique()\n        self.first_fit = True\n\n    def set_fillna(self, v):\n        self.fillna_value = v\n\n    def lst_transform(self, data):\n        result = []\n\n        for val in data:\n            variants = [0] * len(self.columns)\n            not_founded = True\n\n            for i, col in enumerate(self.columns):\n                if val == col:\n                    variants[i] = 1\n                    result.append(variants)\n                    not_founded = False\n                    break\n\n            if not_founded:\n                result.append([self.fillna_value] * len(self.columns))\n        return result\n\n    def transform(self, data):\n        tr_lst = self.lst_transform(data)\n\n        df_data = {f'{self.name}_{col}': [] for col in self.columns}\n\n        for tr in tr_lst:\n            for col, val in zip(self.columns, tr):\n                df_data[f'{self.name}_{col}'].append(val)\n\n        return pd.DataFrame(df_data)\n\n    def __call__(self, data):\n        if self.first_fit == False: \n            self.fit(data)\n            self.first_fit = True\n            \n        return self.transform(data.to_list())","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:58:09.624694Z","iopub.execute_input":"2023-12-30T11:58:09.625156Z","iopub.status.idle":"2023-12-30T11:58:09.636540Z","shell.execute_reply.started":"2023-12-30T11:58:09.625116Z","shell.execute_reply":"2023-12-30T11:58:09.635486Z"},"trusted":true},"execution_count":224,"outputs":[]},{"cell_type":"code","source":"false_true_cols = [\"Sex\", \"Ascites\", \"Spiders\", \"Edema\", \"Hepatomegaly\"]\n\ndrug_enc = GetDummies(name = \"drug\", fit = train_df['Drug'])\n\ndef false_true_cols_(df) -> pd.DataFrame: \n    for i in false_true_cols: \n        if i == 'Sex': \n            df[i] = df[i].apply(lambda l: 1 if l == 'F' else 0)\n        else:  \n            df[i] = df[i].apply(lambda l: 1 if l == 'N' else 0)\n    return df \n\ndef preprocess_y(df): \n    t = {'D': 0, 'C': 1, 'CL': 2}\n    return df.apply(lambda i: t[i])\n\ndef caabstegorical_cols_(df) -> pd.DataFrame: \n    drug_dummies = drug_enc(df['Drug'])\n    df = df.drop('Drug', axis=1)\n    df = pd.concat([drug_dummies, df], axis=1)\n    \n    return df\n\ndef transform_numeric_cols_(df) -> pd.DataFrame:\n    ...\n    return df\n\ndef preprocess_x(df) -> pd.DataFrame: \n    df = df.copy()\n    \n    df = df.drop('id', axis=1)\n    df = transform_numeric_cols_(df)\n    df = false_true_cols_(df)\n    df = categorical_cols_(df)\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:58:09.637654Z","iopub.execute_input":"2023-12-30T11:58:09.637973Z","iopub.status.idle":"2023-12-30T11:58:09.651791Z","shell.execute_reply.started":"2023-12-30T11:58:09.637947Z","shell.execute_reply":"2023-12-30T11:58:09.650472Z"},"trusted":true},"execution_count":225,"outputs":[]},{"cell_type":"code","source":"X = preprocess_x(train_df.drop(['Status'], axis=1))\ny = preprocess_y(train_df['Status'])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, train_size = 0.8)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.695033Z","iopub.execute_input":"2023-12-30T11:59:07.695396Z","iopub.status.idle":"2023-12-30T11:59:07.759181Z","shell.execute_reply.started":"2023-12-30T11:59:07.695368Z","shell.execute_reply":"2023-12-30T11:59:07.758207Z"},"trusted":true},"execution_count":229,"outputs":[]},{"cell_type":"code","source":"from tensorflow.data import Dataset\nimport tensorflow.keras.layers as l\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import Model\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.761260Z","iopub.execute_input":"2023-12-30T11:59:07.761593Z","iopub.status.idle":"2023-12-30T11:59:07.766516Z","shell.execute_reply.started":"2023-12-30T11:59:07.761563Z","shell.execute_reply":"2023-12-30T11:59:07.765601Z"},"trusted":true},"execution_count":230,"outputs":[]},{"cell_type":"code","source":"def build_dataset(X, y, batch_size = 16, shuffle = True, prefetch = True): \n    dataset = Dataset.from_tensor_slices((\n        X, \n        y)\n    ).batch(batch_size)\n    \n    if shuffle: \n        dataset = dataset.shuffle(16)\n        \n    if prefetch: \n        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n        \n    return dataset ","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.767677Z","iopub.execute_input":"2023-12-30T11:59:07.768371Z","iopub.status.idle":"2023-12-30T11:59:07.776809Z","shell.execute_reply.started":"2023-12-30T11:59:07.768335Z","shell.execute_reply":"2023-12-30T11:59:07.775932Z"},"trusted":true},"execution_count":231,"outputs":[]},{"cell_type":"code","source":"def build_model(input_shape, output_shape): \n    inputs = l.Input(input_shape, name='input')\n    \n    x = l.Dropout(0.3)(inputs)\n    \n    x = l.Dense(128, activation = 'linear', name='hidden')(x)\n    x = l.Dense(64, activation = 'linear', name='hidden1')(x)\n#     x = l.BatchNormalization()(inputs)\n\n    outputs = l.Dense(output_shape, activation = 'softmax', name='output')(x)\n    \n    return Model(inputs, outputs)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.779168Z","iopub.execute_input":"2023-12-30T11:59:07.779507Z","iopub.status.idle":"2023-12-30T11:59:07.790746Z","shell.execute_reply.started":"2023-12-30T11:59:07.779479Z","shell.execute_reply":"2023-12-30T11:59:07.789642Z"},"trusted":true},"execution_count":232,"outputs":[]},{"cell_type":"code","source":"train_dataset = build_dataset(X_train, y_train, batch_size = 8)\nval_dataset = build_dataset(X_val, y_val, batch_size = 16)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.792074Z","iopub.execute_input":"2023-12-30T11:59:07.792381Z","iopub.status.idle":"2023-12-30T11:59:07.831837Z","shell.execute_reply.started":"2023-12-30T11:59:07.792355Z","shell.execute_reply":"2023-12-30T11:59:07.830620Z"},"trusted":true},"execution_count":233,"outputs":[]},{"cell_type":"code","source":"# call the model\nmodel = build_model(X_train.shape[-1], 3)\nmodel.compile(optimizer = Adam(1e-4), loss = SparseCategoricalCrossentropy(), metrics=['accuracy'])\n\nes = tf.keras.callbacks.EarlyStopping(patience = 60, min_delta = 1e-5, restore_best_weights = True)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.837823Z","iopub.execute_input":"2023-12-30T11:59:07.838219Z","iopub.status.idle":"2023-12-30T11:59:07.893075Z","shell.execute_reply.started":"2023-12-30T11:59:07.838188Z","shell.execute_reply":"2023-12-30T11:59:07.891885Z"},"trusted":true},"execution_count":234,"outputs":[]},{"cell_type":"code","source":"model.fit(train_dataset, epochs = 200, validation_data = val_dataset, callbacks = [es])","metadata":{"execution":{"iopub.status.busy":"2023-12-30T11:59:07.895423Z","iopub.execute_input":"2023-12-30T11:59:07.895850Z","iopub.status.idle":"2023-12-30T12:06:01.794617Z","shell.execute_reply.started":"2023-12-30T11:59:07.895811Z","shell.execute_reply":"2023-12-30T12:06:01.793469Z"},"trusted":true},"execution_count":235,"outputs":[{"name":"stdout","text":"Epoch 1/200\n791/791 [==============================] - 2s 2ms/step - loss: 75.6140 - accuracy: 0.5530 - val_loss: 28.8357 - val_accuracy: 0.3669\nEpoch 2/200\n791/791 [==============================] - 1s 2ms/step - loss: 41.0524 - accuracy: 0.5639 - val_loss: 16.8353 - val_accuracy: 0.6110\nEpoch 3/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.2061 - accuracy: 0.5803 - val_loss: 23.3225 - val_accuracy: 0.6648\nEpoch 4/200\n791/791 [==============================] - 1s 2ms/step - loss: 36.8717 - accuracy: 0.5769 - val_loss: 41.8019 - val_accuracy: 0.3435\nEpoch 5/200\n791/791 [==============================] - 1s 2ms/step - loss: 38.9043 - accuracy: 0.5748 - val_loss: 28.9863 - val_accuracy: 0.6490\nEpoch 6/200\n791/791 [==============================] - 1s 2ms/step - loss: 29.9340 - accuracy: 0.5829 - val_loss: 51.4904 - val_accuracy: 0.6243\nEpoch 7/200\n791/791 [==============================] - 1s 2ms/step - loss: 33.9356 - accuracy: 0.5954 - val_loss: 47.2614 - val_accuracy: 0.6249\nEpoch 8/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.5181 - accuracy: 0.5870 - val_loss: 12.0258 - val_accuracy: 0.7470\nEpoch 9/200\n791/791 [==============================] - 1s 2ms/step - loss: 35.3949 - accuracy: 0.5952 - val_loss: 9.9064 - val_accuracy: 0.7242\nEpoch 10/200\n791/791 [==============================] - 1s 2ms/step - loss: 34.3035 - accuracy: 0.5946 - val_loss: 36.5790 - val_accuracy: 0.6578\nEpoch 11/200\n791/791 [==============================] - 1s 2ms/step - loss: 30.4950 - accuracy: 0.5974 - val_loss: 7.0578 - val_accuracy: 0.7356\nEpoch 12/200\n791/791 [==============================] - 1s 2ms/step - loss: 33.8622 - accuracy: 0.5976 - val_loss: 25.1954 - val_accuracy: 0.6180\nEpoch 13/200\n791/791 [==============================] - 1s 2ms/step - loss: 34.5519 - accuracy: 0.5887 - val_loss: 23.5254 - val_accuracy: 0.6730\nEpoch 14/200\n791/791 [==============================] - 1s 2ms/step - loss: 33.5808 - accuracy: 0.6029 - val_loss: 12.3624 - val_accuracy: 0.5718\nEpoch 15/200\n791/791 [==============================] - 1s 2ms/step - loss: 28.9421 - accuracy: 0.6093 - val_loss: 31.4221 - val_accuracy: 0.5459\nEpoch 16/200\n791/791 [==============================] - 1s 2ms/step - loss: 32.5336 - accuracy: 0.6200 - val_loss: 14.3000 - val_accuracy: 0.6970\nEpoch 17/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.5845 - accuracy: 0.6063 - val_loss: 102.8319 - val_accuracy: 0.6224\nEpoch 18/200\n791/791 [==============================] - 1s 2ms/step - loss: 44.5274 - accuracy: 0.6004 - val_loss: 23.7460 - val_accuracy: 0.6559\nEpoch 19/200\n791/791 [==============================] - 1s 2ms/step - loss: 27.5797 - accuracy: 0.6189 - val_loss: 6.8230 - val_accuracy: 0.7483\nEpoch 20/200\n791/791 [==============================] - 1s 2ms/step - loss: 29.2713 - accuracy: 0.6085 - val_loss: 8.1442 - val_accuracy: 0.7653\nEpoch 21/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.8584 - accuracy: 0.6029 - val_loss: 11.4375 - val_accuracy: 0.6243\nEpoch 22/200\n791/791 [==============================] - 1s 2ms/step - loss: 30.2852 - accuracy: 0.6069 - val_loss: 23.0346 - val_accuracy: 0.6698\nEpoch 23/200\n791/791 [==============================] - 1s 2ms/step - loss: 24.7626 - accuracy: 0.6252 - val_loss: 21.1160 - val_accuracy: 0.6736\nEpoch 24/200\n791/791 [==============================] - 1s 2ms/step - loss: 28.5495 - accuracy: 0.6165 - val_loss: 9.6236 - val_accuracy: 0.7192\nEpoch 25/200\n791/791 [==============================] - 1s 2ms/step - loss: 33.6214 - accuracy: 0.6112 - val_loss: 11.9178 - val_accuracy: 0.7622\nEpoch 26/200\n791/791 [==============================] - 1s 2ms/step - loss: 29.2532 - accuracy: 0.6137 - val_loss: 37.1055 - val_accuracy: 0.3428\nEpoch 27/200\n791/791 [==============================] - 1s 2ms/step - loss: 28.9399 - accuracy: 0.6199 - val_loss: 141.8312 - val_accuracy: 0.6224\nEpoch 28/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.5648 - accuracy: 0.6229 - val_loss: 46.4486 - val_accuracy: 0.3428\nEpoch 29/200\n791/791 [==============================] - 1s 2ms/step - loss: 31.2808 - accuracy: 0.6055 - val_loss: 22.8312 - val_accuracy: 0.7457\nEpoch 30/200\n791/791 [==============================] - 1s 2ms/step - loss: 29.3035 - accuracy: 0.6222 - val_loss: 13.6553 - val_accuracy: 0.7717\nEpoch 31/200\n791/791 [==============================] - 1s 2ms/step - loss: 24.8876 - accuracy: 0.6298 - val_loss: 10.0700 - val_accuracy: 0.7154\nEpoch 32/200\n791/791 [==============================] - 2s 2ms/step - loss: 28.4643 - accuracy: 0.6107 - val_loss: 99.7737 - val_accuracy: 0.3428\nEpoch 33/200\n791/791 [==============================] - 2s 2ms/step - loss: 30.2983 - accuracy: 0.6099 - val_loss: 37.2963 - val_accuracy: 0.6597\nEpoch 34/200\n791/791 [==============================] - 1s 2ms/step - loss: 25.0160 - accuracy: 0.6202 - val_loss: 13.1497 - val_accuracy: 0.4877\nEpoch 35/200\n791/791 [==============================] - 2s 2ms/step - loss: 28.1758 - accuracy: 0.6137 - val_loss: 10.8958 - val_accuracy: 0.6584\nEpoch 36/200\n791/791 [==============================] - 2s 2ms/step - loss: 27.1035 - accuracy: 0.6200 - val_loss: 32.0113 - val_accuracy: 0.6426\nEpoch 37/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.8590 - accuracy: 0.6101 - val_loss: 8.5019 - val_accuracy: 0.4187\nEpoch 38/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.4157 - accuracy: 0.6142 - val_loss: 15.1550 - val_accuracy: 0.6616\nEpoch 39/200\n791/791 [==============================] - 1s 2ms/step - loss: 29.6820 - accuracy: 0.6159 - val_loss: 27.1535 - val_accuracy: 0.7040\nEpoch 40/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.1266 - accuracy: 0.6259 - val_loss: 44.2251 - val_accuracy: 0.6306\nEpoch 41/200\n791/791 [==============================] - 1s 2ms/step - loss: 28.1952 - accuracy: 0.6175 - val_loss: 32.9923 - val_accuracy: 0.6401\nEpoch 42/200\n791/791 [==============================] - 1s 2ms/step - loss: 27.8119 - accuracy: 0.6210 - val_loss: 19.7020 - val_accuracy: 0.7362\nEpoch 43/200\n791/791 [==============================] - 1s 2ms/step - loss: 25.4462 - accuracy: 0.6186 - val_loss: 10.8060 - val_accuracy: 0.7634\nEpoch 44/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.6874 - accuracy: 0.6121 - val_loss: 45.1972 - val_accuracy: 0.6420\nEpoch 45/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.5078 - accuracy: 0.6282 - val_loss: 8.5202 - val_accuracy: 0.7552\nEpoch 46/200\n791/791 [==============================] - 1s 2ms/step - loss: 28.2674 - accuracy: 0.6183 - val_loss: 46.6902 - val_accuracy: 0.3454\nEpoch 47/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.5406 - accuracy: 0.6139 - val_loss: 10.4282 - val_accuracy: 0.7483\nEpoch 48/200\n791/791 [==============================] - 1s 2ms/step - loss: 26.3388 - accuracy: 0.6222 - val_loss: 35.4559 - val_accuracy: 0.3321\nEpoch 49/200\n791/791 [==============================] - 1s 2ms/step - loss: 25.3845 - accuracy: 0.6229 - val_loss: 51.2314 - val_accuracy: 0.3523\nEpoch 50/200\n791/791 [==============================] - 2s 2ms/step - loss: 28.3413 - accuracy: 0.6112 - val_loss: 28.2824 - val_accuracy: 0.7230\nEpoch 51/200\n791/791 [==============================] - 2s 2ms/step - loss: 28.2107 - accuracy: 0.6085 - val_loss: 15.7660 - val_accuracy: 0.6641\nEpoch 52/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.1370 - accuracy: 0.6146 - val_loss: 128.9594 - val_accuracy: 0.6224\nEpoch 53/200\n791/791 [==============================] - 1s 2ms/step - loss: 23.8608 - accuracy: 0.6240 - val_loss: 8.2241 - val_accuracy: 0.6287\nEpoch 54/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.8824 - accuracy: 0.6293 - val_loss: 15.6981 - val_accuracy: 0.7660\nEpoch 55/200\n791/791 [==============================] - 2s 2ms/step - loss: 33.4274 - accuracy: 0.6061 - val_loss: 25.9612 - val_accuracy: 0.1594\nEpoch 56/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.9770 - accuracy: 0.6282 - val_loss: 22.0219 - val_accuracy: 0.6521\nEpoch 57/200\n791/791 [==============================] - 1s 2ms/step - loss: 25.6342 - accuracy: 0.6271 - val_loss: 31.7616 - val_accuracy: 0.6458\nEpoch 58/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.7057 - accuracy: 0.6244 - val_loss: 6.7821 - val_accuracy: 0.7546\nEpoch 59/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.4543 - accuracy: 0.6333 - val_loss: 9.2518 - val_accuracy: 0.6679\nEpoch 60/200\n791/791 [==============================] - 2s 2ms/step - loss: 28.4299 - accuracy: 0.6327 - val_loss: 14.4590 - val_accuracy: 0.7141\nEpoch 61/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.0287 - accuracy: 0.6164 - val_loss: 37.3496 - val_accuracy: 0.6616\nEpoch 62/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.3681 - accuracy: 0.6316 - val_loss: 28.7042 - val_accuracy: 0.6711\nEpoch 63/200\n791/791 [==============================] - 2s 2ms/step - loss: 27.4424 - accuracy: 0.6216 - val_loss: 42.7218 - val_accuracy: 0.4352\nEpoch 64/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.4408 - accuracy: 0.6219 - val_loss: 9.2447 - val_accuracy: 0.6471\nEpoch 65/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.9165 - accuracy: 0.6072 - val_loss: 7.6864 - val_accuracy: 0.7647\nEpoch 66/200\n791/791 [==============================] - 2s 2ms/step - loss: 20.7228 - accuracy: 0.6335 - val_loss: 16.9576 - val_accuracy: 0.7767\nEpoch 67/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.0569 - accuracy: 0.6303 - val_loss: 10.1289 - val_accuracy: 0.7596\nEpoch 68/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.8673 - accuracy: 0.6292 - val_loss: 25.1861 - val_accuracy: 0.3694\nEpoch 69/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.6912 - accuracy: 0.6203 - val_loss: 9.2668 - val_accuracy: 0.7565\nEpoch 70/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.6570 - accuracy: 0.6270 - val_loss: 32.1642 - val_accuracy: 0.6521\nEpoch 71/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.4790 - accuracy: 0.6237 - val_loss: 17.2224 - val_accuracy: 0.7147\nEpoch 72/200\n791/791 [==============================] - 2s 2ms/step - loss: 26.5002 - accuracy: 0.6278 - val_loss: 14.6179 - val_accuracy: 0.6825\nEpoch 73/200\n791/791 [==============================] - 2s 2ms/step - loss: 29.1792 - accuracy: 0.6151 - val_loss: 69.7267 - val_accuracy: 0.3428\nEpoch 74/200\n791/791 [==============================] - 2s 2ms/step - loss: 30.5959 - accuracy: 0.6256 - val_loss: 21.2481 - val_accuracy: 0.5168\nEpoch 75/200\n791/791 [==============================] - 2s 2ms/step - loss: 27.1232 - accuracy: 0.6222 - val_loss: 22.2277 - val_accuracy: 0.3985\nEpoch 76/200\n791/791 [==============================] - 2s 2ms/step - loss: 27.4155 - accuracy: 0.6205 - val_loss: 20.4504 - val_accuracy: 0.7211\nEpoch 77/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.2182 - accuracy: 0.6256 - val_loss: 7.9150 - val_accuracy: 0.7154\nEpoch 78/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.0730 - accuracy: 0.6422 - val_loss: 28.6914 - val_accuracy: 0.4497\nEpoch 79/200\n791/791 [==============================] - 2s 2ms/step - loss: 23.2614 - accuracy: 0.6343 - val_loss: 37.9151 - val_accuracy: 0.6369\nEpoch 80/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.7748 - accuracy: 0.6203 - val_loss: 22.9529 - val_accuracy: 0.6837\nEpoch 81/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.9817 - accuracy: 0.6282 - val_loss: 24.2046 - val_accuracy: 0.7040\nEpoch 82/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.8643 - accuracy: 0.6301 - val_loss: 11.7663 - val_accuracy: 0.7495\nEpoch 83/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.5836 - accuracy: 0.6314 - val_loss: 13.6671 - val_accuracy: 0.7331\nEpoch 84/200\n791/791 [==============================] - 2s 2ms/step - loss: 25.8454 - accuracy: 0.6230 - val_loss: 18.1727 - val_accuracy: 0.6913\nEpoch 85/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.8324 - accuracy: 0.6325 - val_loss: 39.4702 - val_accuracy: 0.3428\nEpoch 86/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.9847 - accuracy: 0.6176 - val_loss: 14.3801 - val_accuracy: 0.7691\nEpoch 87/200\n791/791 [==============================] - 2s 2ms/step - loss: 20.7736 - accuracy: 0.6369 - val_loss: 18.2185 - val_accuracy: 0.7615\nEpoch 88/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.5199 - accuracy: 0.6415 - val_loss: 11.0430 - val_accuracy: 0.7343\nEpoch 89/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.8013 - accuracy: 0.6330 - val_loss: 24.0536 - val_accuracy: 0.4813\nEpoch 90/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.5008 - accuracy: 0.6325 - val_loss: 30.5272 - val_accuracy: 0.4231\nEpoch 91/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.5762 - accuracy: 0.6317 - val_loss: 32.2737 - val_accuracy: 0.6376\nEpoch 92/200\n791/791 [==============================] - 2s 2ms/step - loss: 23.4493 - accuracy: 0.6287 - val_loss: 31.4128 - val_accuracy: 0.6395\nEpoch 93/200\n791/791 [==============================] - 2s 2ms/step - loss: 24.1053 - accuracy: 0.6110 - val_loss: 38.2439 - val_accuracy: 0.6331\nEpoch 94/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.7548 - accuracy: 0.6399 - val_loss: 30.1626 - val_accuracy: 0.3960\nEpoch 95/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.5551 - accuracy: 0.6229 - val_loss: 13.6979 - val_accuracy: 0.7394\nEpoch 96/200\n791/791 [==============================] - 1s 2ms/step - loss: 22.1253 - accuracy: 0.6213 - val_loss: 11.0889 - val_accuracy: 0.6534\nEpoch 97/200\n791/791 [==============================] - 1s 2ms/step - loss: 23.8436 - accuracy: 0.6293 - val_loss: 18.0110 - val_accuracy: 0.7470\nEpoch 98/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.8804 - accuracy: 0.6347 - val_loss: 33.1396 - val_accuracy: 0.3542\nEpoch 99/200\n791/791 [==============================] - 1s 2ms/step - loss: 23.4529 - accuracy: 0.6303 - val_loss: 8.4810 - val_accuracy: 0.7805\nEpoch 100/200\n791/791 [==============================] - 1s 2ms/step - loss: 25.3422 - accuracy: 0.6216 - val_loss: 44.9622 - val_accuracy: 0.6350\nEpoch 101/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.9835 - accuracy: 0.6331 - val_loss: 5.4086 - val_accuracy: 0.7704\nEpoch 102/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.7738 - accuracy: 0.6303 - val_loss: 13.8069 - val_accuracy: 0.3567\nEpoch 103/200\n791/791 [==============================] - 2s 2ms/step - loss: 19.5670 - accuracy: 0.6404 - val_loss: 16.7279 - val_accuracy: 0.7027\nEpoch 104/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.6210 - accuracy: 0.6369 - val_loss: 18.0386 - val_accuracy: 0.6730\nEpoch 105/200\n791/791 [==============================] - 1s 2ms/step - loss: 19.1207 - accuracy: 0.6328 - val_loss: 11.0725 - val_accuracy: 0.5402\nEpoch 106/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.6163 - accuracy: 0.6336 - val_loss: 9.2412 - val_accuracy: 0.7255\nEpoch 107/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.2087 - accuracy: 0.6279 - val_loss: 10.5349 - val_accuracy: 0.7615\nEpoch 108/200\n791/791 [==============================] - 1s 2ms/step - loss: 19.0242 - accuracy: 0.6335 - val_loss: 5.4057 - val_accuracy: 0.7723\nEpoch 109/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.9204 - accuracy: 0.6382 - val_loss: 17.8980 - val_accuracy: 0.7008\nEpoch 110/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.2550 - accuracy: 0.6289 - val_loss: 20.2230 - val_accuracy: 0.3978\nEpoch 111/200\n791/791 [==============================] - 1s 2ms/step - loss: 23.6974 - accuracy: 0.6338 - val_loss: 10.8877 - val_accuracy: 0.6945\nEpoch 112/200\n791/791 [==============================] - 1s 2ms/step - loss: 24.4714 - accuracy: 0.6311 - val_loss: 20.9335 - val_accuracy: 0.5281\nEpoch 113/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.3842 - accuracy: 0.6325 - val_loss: 6.8801 - val_accuracy: 0.7445\nEpoch 114/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.5467 - accuracy: 0.6369 - val_loss: 26.8304 - val_accuracy: 0.6654\nEpoch 115/200\n791/791 [==============================] - 2s 2ms/step - loss: 19.8569 - accuracy: 0.6211 - val_loss: 10.2425 - val_accuracy: 0.5313\nEpoch 116/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.7657 - accuracy: 0.6259 - val_loss: 28.6379 - val_accuracy: 0.3536\nEpoch 117/200\n791/791 [==============================] - 2s 2ms/step - loss: 18.9954 - accuracy: 0.6292 - val_loss: 14.9341 - val_accuracy: 0.7343\nEpoch 118/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.6144 - accuracy: 0.6324 - val_loss: 8.9341 - val_accuracy: 0.7432\nEpoch 119/200\n791/791 [==============================] - 2s 2ms/step - loss: 21.0729 - accuracy: 0.6251 - val_loss: 18.7193 - val_accuracy: 0.6863\nEpoch 120/200\n791/791 [==============================] - 2s 2ms/step - loss: 22.0943 - accuracy: 0.6301 - val_loss: 4.2322 - val_accuracy: 0.7634\nEpoch 121/200\n791/791 [==============================] - 2s 2ms/step - loss: 20.1068 - accuracy: 0.6327 - val_loss: 12.5104 - val_accuracy: 0.6686\nEpoch 122/200\n791/791 [==============================] - 2s 2ms/step - loss: 18.2350 - accuracy: 0.6376 - val_loss: 10.0276 - val_accuracy: 0.7034\nEpoch 123/200\n791/791 [==============================] - 2s 2ms/step - loss: 19.7877 - accuracy: 0.6414 - val_loss: 25.1494 - val_accuracy: 0.7097\nEpoch 124/200\n791/791 [==============================] - 2s 2ms/step - loss: 19.6647 - accuracy: 0.6295 - val_loss: 26.4935 - val_accuracy: 0.6591\nEpoch 125/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.8367 - accuracy: 0.6338 - val_loss: 11.2092 - val_accuracy: 0.6711\nEpoch 126/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.5286 - accuracy: 0.6244 - val_loss: 65.1488 - val_accuracy: 0.3498\nEpoch 127/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.0049 - accuracy: 0.6233 - val_loss: 7.2551 - val_accuracy: 0.7223\nEpoch 128/200\n791/791 [==============================] - 1s 2ms/step - loss: 17.1029 - accuracy: 0.6324 - val_loss: 18.1291 - val_accuracy: 0.7489\nEpoch 129/200\n791/791 [==============================] - 2s 2ms/step - loss: 17.5835 - accuracy: 0.6376 - val_loss: 20.6784 - val_accuracy: 0.6907\nEpoch 130/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.9491 - accuracy: 0.6252 - val_loss: 17.3081 - val_accuracy: 0.6705\nEpoch 131/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.2035 - accuracy: 0.6355 - val_loss: 7.2551 - val_accuracy: 0.7514\nEpoch 132/200\n791/791 [==============================] - 1s 2ms/step - loss: 20.0712 - accuracy: 0.6262 - val_loss: 50.7136 - val_accuracy: 0.6237\nEpoch 133/200\n791/791 [==============================] - 1s 2ms/step - loss: 19.1530 - accuracy: 0.6335 - val_loss: 26.6146 - val_accuracy: 0.3441\nEpoch 134/200\n791/791 [==============================] - 1s 2ms/step - loss: 19.4868 - accuracy: 0.6346 - val_loss: 24.6622 - val_accuracy: 0.6540\nEpoch 135/200\n791/791 [==============================] - 1s 2ms/step - loss: 21.1047 - accuracy: 0.6328 - val_loss: 18.5186 - val_accuracy: 0.5699\nEpoch 136/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.8657 - accuracy: 0.6308 - val_loss: 4.9639 - val_accuracy: 0.7685\nEpoch 137/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.3020 - accuracy: 0.6352 - val_loss: 12.4084 - val_accuracy: 0.7559\nEpoch 138/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.6715 - accuracy: 0.6346 - val_loss: 11.9481 - val_accuracy: 0.7438\nEpoch 139/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.4818 - accuracy: 0.6316 - val_loss: 9.3569 - val_accuracy: 0.5117\nEpoch 140/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.9502 - accuracy: 0.6396 - val_loss: 31.1995 - val_accuracy: 0.6306\nEpoch 141/200\n791/791 [==============================] - 1s 2ms/step - loss: 17.8429 - accuracy: 0.6344 - val_loss: 17.7813 - val_accuracy: 0.4719\nEpoch 142/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.8244 - accuracy: 0.6331 - val_loss: 37.2242 - val_accuracy: 0.3536\nEpoch 143/200\n791/791 [==============================] - 1s 2ms/step - loss: 17.9619 - accuracy: 0.6373 - val_loss: 32.2539 - val_accuracy: 0.6395\nEpoch 144/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.0916 - accuracy: 0.6354 - val_loss: 11.5225 - val_accuracy: 0.5876\nEpoch 145/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.1231 - accuracy: 0.6366 - val_loss: 10.6876 - val_accuracy: 0.7204\nEpoch 146/200\n791/791 [==============================] - 2s 2ms/step - loss: 17.6198 - accuracy: 0.6406 - val_loss: 9.1027 - val_accuracy: 0.5825\nEpoch 147/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.2284 - accuracy: 0.6301 - val_loss: 13.9963 - val_accuracy: 0.6641\nEpoch 148/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.5406 - accuracy: 0.6395 - val_loss: 13.9136 - val_accuracy: 0.7179\nEpoch 149/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.1521 - accuracy: 0.6325 - val_loss: 7.9612 - val_accuracy: 0.6970\nEpoch 150/200\n791/791 [==============================] - 2s 2ms/step - loss: 17.8877 - accuracy: 0.6331 - val_loss: 14.3399 - val_accuracy: 0.5313\nEpoch 151/200\n791/791 [==============================] - 2s 2ms/step - loss: 16.6454 - accuracy: 0.6331 - val_loss: 7.3748 - val_accuracy: 0.6686\nEpoch 152/200\n791/791 [==============================] - 2s 2ms/step - loss: 16.1555 - accuracy: 0.6379 - val_loss: 27.7406 - val_accuracy: 0.4763\nEpoch 153/200\n791/791 [==============================] - 2s 2ms/step - loss: 14.7982 - accuracy: 0.6410 - val_loss: 13.4296 - val_accuracy: 0.7027\nEpoch 154/200\n791/791 [==============================] - 2s 2ms/step - loss: 13.8031 - accuracy: 0.6388 - val_loss: 10.7490 - val_accuracy: 0.7717\nEpoch 155/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.8477 - accuracy: 0.6437 - val_loss: 17.8116 - val_accuracy: 0.6660\nEpoch 156/200\n791/791 [==============================] - 1s 2ms/step - loss: 17.6368 - accuracy: 0.6298 - val_loss: 10.9813 - val_accuracy: 0.7521\nEpoch 157/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.5684 - accuracy: 0.6336 - val_loss: 7.9454 - val_accuracy: 0.6869\nEpoch 158/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.1488 - accuracy: 0.6322 - val_loss: 6.9643 - val_accuracy: 0.7173\nEpoch 159/200\n791/791 [==============================] - 2s 2ms/step - loss: 17.3464 - accuracy: 0.6281 - val_loss: 8.3123 - val_accuracy: 0.7704\nEpoch 160/200\n791/791 [==============================] - 2s 2ms/step - loss: 16.3143 - accuracy: 0.6346 - val_loss: 3.8655 - val_accuracy: 0.7514\nEpoch 161/200\n791/791 [==============================] - 2s 2ms/step - loss: 14.0483 - accuracy: 0.6297 - val_loss: 10.0362 - val_accuracy: 0.5509\nEpoch 162/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.4679 - accuracy: 0.6365 - val_loss: 6.2577 - val_accuracy: 0.7185\nEpoch 163/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.9304 - accuracy: 0.6333 - val_loss: 6.9743 - val_accuracy: 0.7653\nEpoch 164/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.8381 - accuracy: 0.6382 - val_loss: 5.7601 - val_accuracy: 0.7647\nEpoch 165/200\n791/791 [==============================] - 1s 2ms/step - loss: 17.4865 - accuracy: 0.6297 - val_loss: 4.8154 - val_accuracy: 0.7502\nEpoch 166/200\n791/791 [==============================] - 1s 2ms/step - loss: 18.8569 - accuracy: 0.6287 - val_loss: 4.3903 - val_accuracy: 0.6913\nEpoch 167/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.1174 - accuracy: 0.6273 - val_loss: 8.4880 - val_accuracy: 0.6869\nEpoch 168/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.2536 - accuracy: 0.6429 - val_loss: 14.8167 - val_accuracy: 0.6218\nEpoch 169/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.3054 - accuracy: 0.6330 - val_loss: 6.9334 - val_accuracy: 0.6863\nEpoch 170/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.4407 - accuracy: 0.6230 - val_loss: 39.6432 - val_accuracy: 0.3485\nEpoch 171/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.4095 - accuracy: 0.6306 - val_loss: 64.1546 - val_accuracy: 0.3428\nEpoch 172/200\n791/791 [==============================] - 1s 2ms/step - loss: 16.4304 - accuracy: 0.6244 - val_loss: 24.4954 - val_accuracy: 0.3852\nEpoch 173/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.7629 - accuracy: 0.6305 - val_loss: 6.2840 - val_accuracy: 0.7729\nEpoch 174/200\n791/791 [==============================] - 2s 2ms/step - loss: 14.4571 - accuracy: 0.6335 - val_loss: 19.5860 - val_accuracy: 0.7116\nEpoch 175/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.4752 - accuracy: 0.6377 - val_loss: 10.5336 - val_accuracy: 0.6161\nEpoch 176/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.2144 - accuracy: 0.6240 - val_loss: 18.4070 - val_accuracy: 0.6584\nEpoch 177/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.8500 - accuracy: 0.6312 - val_loss: 10.2144 - val_accuracy: 0.7287\nEpoch 178/200\n791/791 [==============================] - 1s 2ms/step - loss: 19.1764 - accuracy: 0.6175 - val_loss: 19.7738 - val_accuracy: 0.6717\nEpoch 179/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.0023 - accuracy: 0.6403 - val_loss: 8.4047 - val_accuracy: 0.7666\nEpoch 180/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.5540 - accuracy: 0.6317 - val_loss: 7.3675 - val_accuracy: 0.7489\nEpoch 181/200\n791/791 [==============================] - 2s 2ms/step - loss: 16.2374 - accuracy: 0.6279 - val_loss: 5.4257 - val_accuracy: 0.7419\nEpoch 182/200\n791/791 [==============================] - 2s 2ms/step - loss: 15.4867 - accuracy: 0.6317 - val_loss: 6.9416 - val_accuracy: 0.7413\nEpoch 183/200\n791/791 [==============================] - 1s 2ms/step - loss: 13.7268 - accuracy: 0.6374 - val_loss: 4.1053 - val_accuracy: 0.7527\nEpoch 184/200\n791/791 [==============================] - 1s 2ms/step - loss: 13.6629 - accuracy: 0.6335 - val_loss: 9.1081 - val_accuracy: 0.5218\nEpoch 185/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.8990 - accuracy: 0.6412 - val_loss: 7.1722 - val_accuracy: 0.7590\nEpoch 186/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.0968 - accuracy: 0.6305 - val_loss: 8.3025 - val_accuracy: 0.6705\nEpoch 187/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.9118 - accuracy: 0.6361 - val_loss: 6.0795 - val_accuracy: 0.6869\nEpoch 188/200\n791/791 [==============================] - 1s 2ms/step - loss: 12.7804 - accuracy: 0.6384 - val_loss: 12.6565 - val_accuracy: 0.6654\nEpoch 189/200\n791/791 [==============================] - 1s 2ms/step - loss: 13.1904 - accuracy: 0.6403 - val_loss: 12.7246 - val_accuracy: 0.7135\nEpoch 190/200\n791/791 [==============================] - 1s 2ms/step - loss: 12.8552 - accuracy: 0.6407 - val_loss: 8.3134 - val_accuracy: 0.1879\nEpoch 191/200\n791/791 [==============================] - 1s 2ms/step - loss: 12.7129 - accuracy: 0.6232 - val_loss: 8.2657 - val_accuracy: 0.7394\nEpoch 192/200\n791/791 [==============================] - 1s 2ms/step - loss: 15.4044 - accuracy: 0.6306 - val_loss: 4.1246 - val_accuracy: 0.7634\nEpoch 193/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.2382 - accuracy: 0.6346 - val_loss: 23.5167 - val_accuracy: 0.6439\nEpoch 194/200\n791/791 [==============================] - 1s 2ms/step - loss: 12.9100 - accuracy: 0.6387 - val_loss: 4.0440 - val_accuracy: 0.7540\nEpoch 195/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.8004 - accuracy: 0.6252 - val_loss: 13.0450 - val_accuracy: 0.7685\nEpoch 196/200\n791/791 [==============================] - 1s 2ms/step - loss: 14.0345 - accuracy: 0.6300 - val_loss: 5.1030 - val_accuracy: 0.7729\nEpoch 197/200\n791/791 [==============================] - 1s 2ms/step - loss: 11.5938 - accuracy: 0.6504 - val_loss: 15.5363 - val_accuracy: 0.4016\nEpoch 198/200\n791/791 [==============================] - 1s 2ms/step - loss: 13.9568 - accuracy: 0.6322 - val_loss: 4.8252 - val_accuracy: 0.7641\nEpoch 199/200\n791/791 [==============================] - 1s 2ms/step - loss: 12.5126 - accuracy: 0.6434 - val_loss: 13.3066 - val_accuracy: 0.7521\nEpoch 200/200\n791/791 [==============================] - 2s 2ms/step - loss: 14.6230 - accuracy: 0.6339 - val_loss: 19.8587 - val_accuracy: 0.6831\n","output_type":"stream"},{"execution_count":235,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.History at 0x786404631d20>"},"metadata":{}}]},{"cell_type":"code","source":"test_df = pd.read_csv(test_path)\ntest_X = preprocess_x(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T12:09:57.218609Z","iopub.execute_input":"2023-12-30T12:09:57.219011Z","iopub.status.idle":"2023-12-30T12:09:57.271959Z","shell.execute_reply.started":"2023-12-30T12:09:57.218981Z","shell.execute_reply":"2023-12-30T12:09:57.271072Z"},"trusted":true},"execution_count":240,"outputs":[]},{"cell_type":"code","source":"prediction = model.predict(test_X)\nc_status = prediction[:, 1]\ncl_status = prediction[:, 2]\nd_status = prediction[:, 0]\n\ndf_data = {\n    'Status_C': c_status, \n    'Status_CL': cl_status,\n    'Status_D': d_status, \n}","metadata":{"execution":{"iopub.status.busy":"2023-12-30T12:09:58.446092Z","iopub.execute_input":"2023-12-30T12:09:58.446782Z","iopub.status.idle":"2023-12-30T12:09:58.772947Z","shell.execute_reply.started":"2023-12-30T12:09:58.446749Z","shell.execute_reply":"2023-12-30T12:09:58.771867Z"},"trusted":true},"execution_count":241,"outputs":[{"name":"stdout","text":"165/165 [==============================] - 0s 1ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"submission = pd.DataFrame({'id': test_df['id'], **df_data})","metadata":{"execution":{"iopub.status.busy":"2023-12-30T12:10:01.546248Z","iopub.execute_input":"2023-12-30T12:10:01.546660Z","iopub.status.idle":"2023-12-30T12:10:01.552256Z","shell.execute_reply.started":"2023-12-30T12:10:01.546626Z","shell.execute_reply":"2023-12-30T12:10:01.551342Z"},"trusted":true},"execution_count":242,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T12:10:02.978691Z","iopub.execute_input":"2023-12-30T12:10:02.979346Z","iopub.status.idle":"2023-12-30T12:10:03.009094Z","shell.execute_reply.started":"2023-12-30T12:10:02.979308Z","shell.execute_reply":"2023-12-30T12:10:03.007804Z"},"trusted":true},"execution_count":243,"outputs":[]}]}